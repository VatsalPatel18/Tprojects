{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0313b973-6089-4f68-9619-00ce13d19003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "config = {\n",
    "    \"NUMERIC_FEATURE_NAMES\": [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"],\n",
    "    \"CATEGORICAL_FEATURE_NAMES\": [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native_country\"],\n",
    "    \"TARGET_FEATURE_NAME\": \"income_bracket\",\n",
    "    \"TARGET_LABELS\": [\" <=50K\", \" >50K\"],\n",
    "    \"EMBEDDING_DIMS\": 16,\n",
    "    \"NUM_TRANSFORMER_BLOCKS\": 3,\n",
    "    \"NUM_HEADS\": 4,\n",
    "    \"DROPOUT_RATE\": 0.2,\n",
    "    \"MLP_HIDDEN_UNITS_FACTORS\": [2, 1],\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"BATCH_SIZE\": 265,\n",
    "    \"NUM_EPOCHS\": 15,\n",
    "    # \"TRAIN_DATA_URL\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    # \"TEST_DATA_URL\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "    # \"CSV_HEADER\": [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income_bracket\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e48fd1-01b6-4cde-992b-d2e53adf4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (4000, 14)\n",
      "Test dataset shape: (1000, 14)\n"
     ]
    }
   ],
   "source": [
    "# data_preparation.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_data(num_samples=5000):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Generating random numerical data\n",
    "    age = np.random.randint(18, 70, num_samples)\n",
    "    education_num = np.random.randint(1, 16, num_samples)\n",
    "    capital_gain = np.random.randint(0, 10000, num_samples)\n",
    "    capital_loss = np.random.randint(0, 5000, num_samples)\n",
    "    hours_per_week = np.random.randint(1, 99, num_samples)\n",
    "    \n",
    "    # Generating random categorical data\n",
    "    workclass = np.random.choice(['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked'], num_samples)\n",
    "    education = np.random.choice(['Bachelors', 'Some-college', '11th', 'HS-grad', 'Prof-school', 'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th', '12th', 'Masters', '1st-4th', '10th', 'Doctorate', '5th-6th', 'Preschool'], num_samples)\n",
    "    marital_status = np.random.choice(['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'], num_samples)\n",
    "    occupation = np.random.choice(['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces'], num_samples)\n",
    "    relationship = np.random.choice(['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'], num_samples)\n",
    "    race = np.random.choice(['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black'], num_samples)\n",
    "    gender = np.random.choice(['Male', 'Female'], num_samples)\n",
    "    native_country = np.random.choice(['United-States', 'Cambodia', 'England', 'Puerto-Rico', 'Canada', 'Germany', 'Outlying-US(Guam-USVI-etc)', 'India', 'Japan', 'Greece', 'South', 'China', 'Cuba', 'Iran', 'Honduras', 'Philippines', 'Italy', 'Poland', 'Jamaica', 'Vietnam', 'Mexico', 'Portugal', 'Ireland', 'France', 'Dominican-Republic', 'Laos', 'Ecuador', 'Taiwan', 'Haiti', 'Columbia', 'Hungary', 'Guatemala', 'Nicaragua', 'Scotland', 'Thailand', 'Yugoslavia', 'El-Salvador', 'Trinadad&Tobago', 'Peru', 'Hong', 'Holand-Netherlands'], num_samples)\n",
    "    \n",
    "    # Generating random target variable\n",
    "    income_bracket = np.random.choice([' <=50K', ' >50K'], num_samples)\n",
    "    \n",
    "    # Combining all features into a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'workclass': workclass,\n",
    "        'education_num': education_num,\n",
    "        'education': education,\n",
    "        'marital_status': marital_status,\n",
    "        'occupation': occupation,\n",
    "        'relationship': relationship,\n",
    "        'race': race,\n",
    "        'gender': gender,\n",
    "        'capital_gain': capital_gain,\n",
    "        'capital_loss': capital_loss,\n",
    "        'hours_per_week': hours_per_week,\n",
    "        'native_country': native_country,\n",
    "        'income_bracket': income_bracket\n",
    "    })\n",
    "    \n",
    "    # Splitting the data into train and test sets\n",
    "    train_data = data.sample(frac=0.8, random_state=42)\n",
    "    test_data = data.drop(train_data.index)\n",
    "    \n",
    "    # Saving to CSV files\n",
    "    train_data.to_csv(\"train_data.csv\", index=False, header=False)\n",
    "    test_data.to_csv(\"test_data.csv\", index=False, header=False)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data, test_data = generate_random_data()\n",
    "    print(f\"Train dataset shape: {train_data.shape}\")\n",
    "    print(f\"Test dataset shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5d0399-6a1a-4df7-92dc-0bc572b1d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb75b0-c7ce-4ffa-adc1-1e010da38710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0baab9-ea42-4d99-8d6d-501f28f9b26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe53c72f-c74a-4958-ad43-cd82acb13f23",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m     train_model(model, train_loader, test_loader, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_EPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWEIGHT_DECAY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[15], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m model \u001b[38;5;241m=\u001b[39m TabTransformer(num_categories, \u001b[38;5;28mlen\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUMERIC_FEATURE_NAMES\u001b[39m\u001b[38;5;124m\"\u001b[39m]), config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEMBEDDING_DIMS\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_HEADS\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_TRANSFORMER_BLOCKS\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROPOUT_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP_HIDDEN_UNITS_FACTORS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m train_model(model, train_loader, test_loader, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_EPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWEIGHT_DECAY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, weight_decay)\u001b[0m\n\u001b[1;32m     87\u001b[0m categorical_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     88\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 90\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(numerical_data, categorical_data)\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     93\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[0;34m(self, numerical_data, categorical_data)\u001b[0m\n\u001b[1;32m     68\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [embed(categorical_data[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, embed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)]\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# tabtransformer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from config import config\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, numerical_features, categorical_features, target):\n",
    "        self.numerical_data = data[numerical_features].values.astype(np.float32)\n",
    "        self.categorical_data = data[categorical_features].apply(lambda x: x.astype('category').cat.codes).values\n",
    "        self.labels = LabelEncoder().fit_transform(data[target])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'numerical_data': torch.tensor(self.numerical_data[idx]),\n",
    "            'categorical_data': torch.tensor(self.categorical_data[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(feed_forward_output))\n",
    "        return x\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, num_categories, num_numerical, embed_size, num_heads, num_blocks, dropout, mlp_hidden_units_factors):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, embed_size) for categories in num_categories])\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(embed_size, num_heads, dropout) for _ in range(num_blocks)])\n",
    "        self.mlp_input_size = len(num_categories) * embed_size + num_numerical\n",
    "        mlp_hidden_units = [factor * self.mlp_input_size for factor in mlp_hidden_units_factors]\n",
    "        self.mlp = self._create_mlp(mlp_hidden_units, dropout)\n",
    "        self.output_layer = nn.Linear(mlp_hidden_units[-1], 1)\n",
    "        \n",
    "    def _create_mlp(self, hidden_units, dropout):\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(self.mlp_input_size, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            self.mlp_input_size = units\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, numerical_data, categorical_data):\n",
    "        embeddings = [embed(categorical_data[:, i]) for i, embed in enumerate(self.embeddings)]\n",
    "        x = torch.stack(embeddings, dim=1)  # Stack along the new dimension\n",
    "        x = x.permute(1, 0, 2)  # Permute to (sequence_length, batch_size, embed_dim)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(1, 0, 2).contiguous().view(x.size(0), -1)  # Reshape back\n",
    "        x = torch.cat([x, numerical_data], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, weight_decay):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            numerical_data = batch['numerical_data']\n",
    "            categorical_data = batch['categorical_data']\n",
    "            labels = batch['label'].unsqueeze(1).float()\n",
    "            \n",
    "            outputs = model(numerical_data, categorical_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                numerical_data = batch['numerical_data']\n",
    "                categorical_data = batch['categorical_data']\n",
    "                labels = batch['label'].unsqueeze(1).float()\n",
    "                \n",
    "                outputs = model(numerical_data, categorical_data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                val_acc += (predictions == labels).float().mean()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc/len(val_loader):.4f}')\n",
    "\n",
    "def main():\n",
    "    # Generate random dataset\n",
    "    train_data, test_data = generate_random_data()\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = TabularDataset(train_data, config[\"NUMERIC_FEATURE_NAMES\"], config[\"CATEGORICAL_FEATURE_NAMES\"], config[\"TARGET_FEATURE_NAME\"])\n",
    "    test_dataset = TabularDataset(test_data, config[\"NUMERIC_FEATURE_NAMES\"], config[\"CATEGORICAL_FEATURE_NAMES\"], config[\"TARGET_FEATURE_NAME\"])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)\n",
    "    \n",
    "    # Determine number of unique categories for each categorical feature\n",
    "    num_categories = [len(train_data[feature].unique()) for feature in config[\"CATEGORICAL_FEATURE_NAMES\"]]\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = TabTransformer(num_categories, len(config[\"NUMERIC_FEATURE_NAMES\"]), config[\"EMBEDDING_DIMS\"], config[\"NUM_HEADS\"], config[\"NUM_TRANSFORMER_BLOCKS\"], config[\"DROPOUT_RATE\"], config[\"MLP_HIDDEN_UNITS_FACTORS\"])\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, test_loader, config[\"NUM_EPOCHS\"], config[\"LEARNING_RATE\"], config[\"WEIGHT_DECAY\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c1d19-4dde-4509-a956-2e63c4b27b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
