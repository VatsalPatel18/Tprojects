{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98617520-b686-45a3-a419-8924a8e89b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': {\n",
    "        'embedding_dims': 16,\n",
    "        'num_transformer_blocks': 3,\n",
    "        'num_heads': 4,\n",
    "        'mlp_hidden_units_factors': [2, 1],\n",
    "        'dropout_rate': 0.2,\n",
    "        'use_column_embedding': False\n",
    "    },\n",
    "    'training': {\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0001,\n",
    "        'batch_size': 265,\n",
    "        'num_epochs': 15\n",
    "    },\n",
    "    'data': {\n",
    "        'train_data_file': \"train_data.csv\",\n",
    "        'test_data_file': \"test_data.csv\",\n",
    "        'csv_header': [\n",
    "            \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \n",
    "            \"marital_status\", \"occupation\", \"relationship\", \"race\", \n",
    "            \"gender\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \n",
    "            \"native_country\", \"income_bracket\"\n",
    "        ],\n",
    "        'target_labels': [\" <=50K\", \" >50K\"],\n",
    "        'weight_column_name': \"fnlwgt\",\n",
    "        'numerical_feature_names': [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"],\n",
    "        'categorical_feature_names': [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fdf2541-9cef-4972-a328-39298b6569d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import pandas as pd\n",
    "from tensorflow import data as tf_data\n",
    "from functools import partial\n",
    "\n",
    "class TabTransformer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._prepare_metadata()\n",
    "\n",
    "    def _prepare_metadata(self):\n",
    "        self.csv_header = self.config['data']['csv_header']\n",
    "        self.target_labels = self.config['data']['target_labels']\n",
    "        self.weight_column_name = self.config['data']['weight_column_name']\n",
    "        self.numerical_feature_names = self.config['data']['numerical_feature_names']\n",
    "        self.categorical_feature_names = self.config['data']['categorical_feature_names']\n",
    "        self.categorical_features_with_vocabulary = self._get_categorical_features_vocab()\n",
    "        self.feature_names = self.numerical_feature_names + self.categorical_feature_names\n",
    "\n",
    "    def _get_categorical_features_vocab(self):\n",
    "        train_data = pd.read_csv(self.config['data']['train_data_file'], header=None, names=self.csv_header)\n",
    "        vocab = {}\n",
    "        for feature_name in self.categorical_feature_names:\n",
    "            vocab[feature_name] = sorted(train_data[feature_name].unique())\n",
    "        return vocab\n",
    "\n",
    "    def create_model_inputs(self):\n",
    "        inputs = {}\n",
    "        for feature_name in self.feature_names:\n",
    "            if feature_name in self.numerical_feature_names:\n",
    "                inputs[feature_name] = layers.Input(name=feature_name, shape=(), dtype=\"float32\")\n",
    "            else:\n",
    "                inputs[feature_name] = layers.Input(name=feature_name, shape=(), dtype=\"int32\")\n",
    "        return inputs\n",
    "\n",
    "    def encode_inputs(self, inputs, embedding_dims):\n",
    "        encoded_categorical_feature_list = []\n",
    "        numerical_feature_list = []\n",
    "\n",
    "        for feature_name in inputs:\n",
    "            if feature_name in self.categorical_feature_names:\n",
    "                vocabulary = self.categorical_features_with_vocabulary[feature_name]\n",
    "                embedding = layers.Embedding(input_dim=len(vocabulary), output_dim=embedding_dims)\n",
    "                encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "                encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "            else:\n",
    "                numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "                numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "        return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "    def create_mlp(self, hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "        mlp_layers = []\n",
    "        for units in hidden_units:\n",
    "            mlp_layers.append(normalization_layer())\n",
    "            mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "            mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "        return keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "    def create_tabtransformer_classifier(self):\n",
    "        embedding_dims = self.config['model']['embedding_dims']\n",
    "        num_transformer_blocks = self.config['model']['num_transformer_blocks']\n",
    "        num_heads = self.config['model']['num_heads']\n",
    "        mlp_hidden_units_factors = self.config['model']['mlp_hidden_units_factors']\n",
    "        dropout_rate = self.config['model']['dropout_rate']\n",
    "        use_column_embedding = self.config['model']['use_column_embedding']\n",
    "\n",
    "        inputs = self.create_model_inputs()\n",
    "        encoded_categorical_feature_list, numerical_feature_list = self.encode_inputs(inputs, embedding_dims)\n",
    "        encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "        numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "        if use_column_embedding:\n",
    "            num_columns = encoded_categorical_features.shape[1]\n",
    "            column_embedding = layers.Embedding(input_dim=num_columns, output_dim=embedding_dims)\n",
    "            column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "            encoded_categorical_features = encoded_categorical_features + column_embedding(column_indices)\n",
    "\n",
    "        for block_idx in range(num_transformer_blocks):\n",
    "            attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims, dropout=dropout_rate, name=f\"multihead_attention_{block_idx}\")(encoded_categorical_features, encoded_categorical_features)\n",
    "            x = layers.Add(name=f\"skip_connection1_{block_idx}\")([attention_output, encoded_categorical_features])\n",
    "            x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "            feedforward_output = self.create_mlp(hidden_units=[embedding_dims], dropout_rate=dropout_rate, activation=keras.activations.gelu, normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6), name=f\"feedforward_{block_idx}\")(x)\n",
    "            x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "            encoded_categorical_features = layers.LayerNormalization(name=f\"layer_norm2_{block_idx}\", epsilon=1e-6)(x)\n",
    "\n",
    "        categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "        numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "        features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "        mlp_hidden_units = [factor * features.shape[-1] for factor in mlp_hidden_units_factors]\n",
    "        features = self.create_mlp(hidden_units=mlp_hidden_units, dropout_rate=dropout_rate, activation=keras.activations.selu, normalization_layer=layers.BatchNormalization, name=\"MLP\")(features)\n",
    "\n",
    "        outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ed5f60-ed96-4538-b48f-40713eb051ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_18218/3085553066.py\", line 28, in None  *\n        lambda features, target, weights: encode_categorical(features, target, weights, config)\n    File \"/tmp/ipykernel_18218/3085553066.py\", line 35, in encode_categorical  *\n        vocabulary = config['data']['categorical_features_with_vocabulary'][feature_name]\n\n    KeyError: 'categorical_features_with_vocabulary'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m tab_transformer \u001b[38;5;241m=\u001b[39m TabTransformer(config)\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m tab_transformer\u001b[38;5;241m.\u001b[39mcreate_tabtransformer_classifier()\n\u001b[0;32m---> 64\u001b[0m history \u001b[38;5;241m=\u001b[39m run_experiment(\n\u001b[1;32m     65\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     66\u001b[0m     train_data_file\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data_file\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m     test_data_file\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data_file\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     68\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     69\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     70\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     71\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     72\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m     73\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model, train_data_file, test_data_file, num_epochs, learning_rate, weight_decay, batch_size, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(), metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mBinaryAccuracy(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m---> 48\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m get_dataset_from_csv(train_data_file, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     49\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m get_dataset_from_csv(test_data_file, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m, in \u001b[0;36mget_dataset_from_csv\u001b[0;34m(csv_file_path, batch_size, shuffle, config)\u001b[0m\n\u001b[1;32m     12\u001b[0m CSV_HEADER \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv_header\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m COLUMN_DEFAULTS \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical_feature_names\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m feature_name \u001b[38;5;129;01min\u001b[39;00m CSV_HEADER]\n\u001b[1;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     tf_data\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mmake_csv_dataset(\n\u001b[1;32m     17\u001b[0m         csv_file_path,\n\u001b[1;32m     18\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     19\u001b[0m         column_names\u001b[38;5;241m=\u001b[39mCSV_HEADER,\n\u001b[1;32m     20\u001b[0m         column_defaults\u001b[38;5;241m=\u001b[39mCOLUMN_DEFAULTS,\n\u001b[1;32m     21\u001b[0m         label_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome_bracket\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     23\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m         na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m features, target: prepare_example(features, target, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m]), num_parallel_calls\u001b[38;5;241m=\u001b[39mtf_data\u001b[38;5;241m.\u001b[39mAUTOTUNE, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m features, target, weights: encode_categorical(features, target, weights, config))\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2299\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[1;32m   2300\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2301\u001b[0m     map_func,\n\u001b[1;32m   2302\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[1;32m   2303\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m   2304\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m structured_function\u001b[38;5;241m.\u001b[39mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[38;5;241m=\u001b[39minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[38;5;241m=\u001b[39muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m fn_factory()\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwargs, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    694\u001b[0m )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[1;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[1;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[1;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[1;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[1;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[1;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[1;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m wrapper_helper(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, ag_ctx)(\u001b[38;5;241m*\u001b[39mnested_args)\n\u001b[1;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileti9uxqbl.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(features, target, weights)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m features, target, weights: ag__\u001b[38;5;241m.\u001b[39mwith_function_scope(\u001b[38;5;28;01mlambda\u001b[39;00m lscope: ag__\u001b[38;5;241m.\u001b[39mconverted_call(encode_categorical, (features, target, weights, config), \u001b[38;5;28;01mNone\u001b[39;00m, lscope), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/core/function_wrappers.py:113\u001b[0m, in \u001b[0;36mwith_function_scope\u001b[0;34m(thunk, scope_name, options)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_\u001b[39m\u001b[38;5;124m'\u001b[39m, scope_name, options) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[0;32m--> 113\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m thunk(scope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileti9uxqbl.py:6\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 6\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m features, target, weights: ag__\u001b[38;5;241m.\u001b[39mwith_function_scope(\u001b[38;5;28;01mlambda\u001b[39;00m lscope: ag__\u001b[38;5;241m.\u001b[39mconverted_call(encode_categorical, (features, target, weights, config), \u001b[38;5;28;01mNone\u001b[39;00m, lscope), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:441\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    439\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filep0054xjr.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_categorical\u001b[0;34m(batch_x, batch_y, weights, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m feature_name \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m lookup \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlookup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mld(config)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_feature_names\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py:449\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iter_, distribute\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# TODO(b/162250181): Use _tf_iterator_for_stmt(iter(iter_)...\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     for_fn \u001b[38;5;241m=\u001b[39m _tf_distributed_iterable_for_stmt\n\u001b[0;32m--> 449\u001b[0m for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py:500\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m iter_:\n\u001b[0;32m--> 500\u001b[0m     body(target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py:466\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 466\u001b[0m   original_body(protected_iter)\n\u001b[1;32m    467\u001b[0m   after_iteration()\n\u001b[1;32m    468\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filep0054xjr.py:20\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_categorical.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloop_body\u001b[39m(itr):\n\u001b[1;32m     19\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m itr\n\u001b[0;32m---> 20\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(config)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_features_with_vocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m][ag__\u001b[38;5;241m.\u001b[39mld(feature_name)]\n\u001b[1;32m     21\u001b[0m     lookup \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(layers)\u001b[38;5;241m.\u001b[39mStringLookup, (), \u001b[38;5;28mdict\u001b[39m(vocabulary\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(vocabulary), mask_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_oov_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n\u001b[1;32m     22\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mld(lookup_dict)[ag__\u001b[38;5;241m.\u001b[39mld(feature_name)] \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(lookup)\n",
      "\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_18218/3085553066.py\", line 28, in None  *\n        lambda features, target, weights: encode_categorical(features, target, weights, config)\n    File \"/tmp/ipykernel_18218/3085553066.py\", line 35, in encode_categorical  *\n        vocabulary = config['data']['categorical_features_with_vocabulary'][feature_name]\n\n    KeyError: 'categorical_features_with_vocabulary'\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "def prepare_example(features, target, weight_column_name):\n",
    "    target_label_lookup = layers.StringLookup(vocabulary=config['data']['target_labels'], mask_token=None, num_oov_indices=0)\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(weight_column_name)\n",
    "    return features, target_index, weights\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size, shuffle, config):\n",
    "    CSV_HEADER = config['data']['csv_header']\n",
    "    COLUMN_DEFAULTS = [[0.0] if feature_name in config['data']['numerical_feature_names'] + [config['data']['weight_column_name']] else [\"NA\"] for feature_name in CSV_HEADER]\n",
    "\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name='income_bracket',\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(lambda features, target: prepare_example(features, target, config['data']['weight_column_name']), num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "        .map(lambda features, target, weights: encode_categorical(features, target, weights, config))\n",
    "    )\n",
    "    return dataset.cache()\n",
    "\n",
    "def encode_categorical(batch_x, batch_y, weights, config):\n",
    "    lookup_dict = {}\n",
    "    for feature_name in config['data']['categorical_feature_names']:\n",
    "        vocabulary = config['data']['categorical_features_with_vocabulary'][feature_name]\n",
    "        lookup = layers.StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)\n",
    "        lookup_dict[feature_name] = lookup\n",
    "\n",
    "    for feature_name in config['data']['categorical_feature_names']:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y, weights\n",
    "\n",
    "def run_experiment(model, train_data_file, test_data_file, num_epochs, learning_rate, weight_decay, batch_size, config):\n",
    "    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True, config=config)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size, shuffle=False, config=config)\n",
    "    \n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n",
    "    print(\"Model training finished\")\n",
    "    \n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Instantiate and run the TabTransformer\n",
    "tab_transformer = TabTransformer(config)\n",
    "model = tab_transformer.create_tabtransformer_classifier()\n",
    "\n",
    "history = run_experiment(\n",
    "    model=model,\n",
    "    train_data_file=config['data']['train_data_file'],\n",
    "    test_data_file=config['data']['test_data_file'],\n",
    "    num_epochs=config['training']['num_epochs'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dceb41c0-b287-46f2-a9d9-58060c7c3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab_transformer = TabTransformer(config)\n",
    "# model = tab_transformer.create_tabtransformer_classifier()\n",
    "\n",
    "# run_experiment(\n",
    "#     model=model,\n",
    "#     train_data_file=config['data']['train_data_file'],\n",
    "#     test_data_file=config['data']['test_data_file'],\n",
    "#     num_epochs=config['training']['num_epochs'],\n",
    "#     learning_rate=config['training']['learning_rate'],\n",
    "#     weight_decay=config['training']['weight_decay'],\n",
    "#     batch_size=config['training']['batch_size'],\n",
    "#     config=config\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5ab5aa0-9714-444a-8958-bb72741c8869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59,Self-emp-inc,12,9th,Divorced,Exec-managerial,Other-relative,Black,Male,5985,869,86,France, <=50K\n",
      "25,Local-gov,9,9th,Divorced,Machine-op-inspct,Wife,Black,Female,9447,625,5,Cambodia, <=50K\n",
      "38,Private,13,1st-4th,Widowed,Exec-managerial,Unmarried,Other,Female,1550,2555,65,Portugal, <=50K\n",
      "48,Federal-gov,7,11th,Never-married,Machine-op-inspct,Other-relative,Other,Male,7001,1240,92,Honduras, >50K\n",
      "53,Local-gov,11,9th,Married-spouse-absent,Transport-moving,Own-child,Black,Female,7420,3793,62,India, <=50K\n",
      "18,Local-gov,8,Prof-school,Widowed,Armed-Forces,Unmarried,Other,Female,4868,2581,87,Greece, >50K\n",
      "43,Private,13,Prof-school,Never-married,Machine-op-inspct,Wife,Amer-Indian-Eskimo,Male,4911,3946,36,Thailand, >50K\n",
      "59,Federal-gov,1,Preschool,Married-civ-spouse,Adm-clerical,Unmarried,White,Female,6431,2795,29,Nicaragua, <=50K\n",
      "25,Federal-gov,7,Assoc-voc,Separated,Protective-serv,Other-relative,Other,Male,3916,111,88,France, >50K\n",
      "41,Federal-gov,2,Masters,Separated,Sales,Unmarried,Other,Female,1306,616,97,Trinadad&Tobago, >50K\n"
     ]
    }
   ],
   "source": [
    "!head data/train_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceef896-e679-4494-a733-8485671e0173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
