


import torch
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments,AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset 



# !pip install datasets


# 1. Load Your Dataset (Assuming CSV or JSON format)
dataset = load_dataset("csv", data_files="gemini_tempdata.csv")  


dataset


tokenizer = AutoTokenizer.from_pretrained("Phi-3-mini-4k-instruct")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)


# 3. Load and Adapt the Phi-3 Mini Model
# model = AutoModelForCausalLM.from_pretrained("Phi-3-mini-4k-instruct")


peft_config = LoraConfig(
    r=8,  # Adjust the rank for efficiency vs. performance tradeoff
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"],  # You can experiment with these
)









model = get_peft_model(model, peft_config)

# 4. Set up Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,  # Adjust based on your GPU/CPU resources
    gradient_accumulation_steps=4,  # Increase if you face OOM issues
    num_train_epochs=3,
    learning_rate=2e-4,  # You can experiment with different values
    logging_steps=50,
    save_strategy="epoch",
)

# 5. Train the Model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],  # Assuming you have a 'train' split in your dataset
)
trainer.train()

# 6. Save the Fine-Tuned Model
model.save_pretrained("./fine_tuned_phi3")  # Save in a suitable directory
