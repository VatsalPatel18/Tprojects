


!pip install -U transformers
!pip install huggingface_hub peft bitsandbytes
!pip install trl xformers flash-attn


!nvidia-smi


import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "2"


from transformers.utils import is_flash_attn_2_available
is_flash_attn_2_available()


def count_model_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")


import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline
MAX_SEQ_LENGTH = 512

config = AutoConfig.from_pretrained("microsoft/Phi-3-mini-128k-instruct", torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation='flash_attention_2')
config.max_position_embeddings = MAX_SEQ_LENGTH   # in the original Phi3-128k this was 131072 (2**17), A*Star sets this to 512. Note this wont change the size of the parameters but does save a LOT VRAM during training.
config.num_hidden_layers = 10
config.tie_word_embeddings = True
config.hidden_size = 512
config.intermediate_size = 1536
config.num_attention_heads = 16
config.num_key_value_heads = 16

# Adjust the rope scaling factors
required_length = config.hidden_size // (config.num_attention_heads * 2)
config.rope_scaling['long_factor'] = config.rope_scaling['long_factor'][:required_length]
config.rope_scaling['short_factor'] = config.rope_scaling['short_factor'][:required_length]


# Initialize a new model with the modified configuration
new_model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation='flash_attention_2')


new_model.config


count_model_parameters(new_model)





# General function to copy tensor with shape handling
def copy_tensor(pre_tensor, new_tensor):
    # Determine the slice indices for each dimension
    slices = tuple(slice(-min(pre_dim, new_dim), None) if pre_dim != new_dim else slice(None)
                   for pre_dim, new_dim in zip(pre_tensor.shape, new_tensor.shape))

    # Copy the relevant sub-tensor
    new_tensor[slices] = pre_tensor[slices]
    return new_tensor

# Function to copy weights with generalized shape handling
def copy_weights(pretrained_model, new_model):
    pretrained_state_dict = pretrained_model.state_dict()
    new_state_dict = new_model.state_dict()

    for key in new_state_dict.keys():
        if key in pretrained_state_dict:
            pre_tensor = pretrained_state_dict[key]
            new_tensor = new_state_dict[key]
            if new_tensor.shape == pre_tensor.shape:
                new_state_dict[key] = pre_tensor
                print(f'{key} get fully copied')
            else:
                new_state_dict[key] = copy_tensor(pre_tensor, new_tensor)
                print(f'{key} get partial copied')
        else:
            print(f"Skipping {key} as it is not present in the pretrained model.")

    new_model.load_state_dict(new_state_dict)
    print('pretrained weights are copied to the new model')
    return new_model

pretrained_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-128k-instruct",
    torch_dtype="auto",
    trust_remote_code=True,
    attn_implementation='flash_attention_2'
)
# Copy weights from the pretrained model to the new model
new_model = copy_weights(pretrained_model, new_model)


tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct")
new_model.to('cuda')


messages = [
    {"role": "user", "content": '''tell me a story'''},
]

generation_args = {
    "max_new_tokens": 500,
    "return_full_text": False,
    "temperature": 0.0,
    "do_sample": False,
}

pipe = pipeline(
    "text-generation",
    model=new_model,
    tokenizer=tokenizer,
)

output = pipe(messages, **generation_args)
print(output[0]['generated_text'])


from datasets import load_dataset

dataset = load_dataset("roneneldan/TinyStories")


dataset


def formatting_prompts_func(story):
    # note that the original tinystories dataset is NOT instruction-followin, so here for convience i just fix the instruction to tell me a story.
    text = f"<|user|>tell me a story<|end|><|assistant|>{story['text']}<|endoftext|>"
    return {"text": text}


from transformers import TrainingArguments

per_device_train_batch_size = 64  # adjust this if u r running on bigger/smaller VRAM, for 3090 24GB this seems fine
gradient_accumulation_steps = 2
num_train_epochs = 1

args = TrainingArguments(
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    gradient_checkpointing=True,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    max_steps=-1,
    num_train_epochs=num_train_epochs,
    evaluation_strategy="steps",
    logging_steps=100,
    output_dir='phi-3-tinystories',
    optim="paged_adamw_32bit",
    bf16=True,
)


from trl import SFTTrainer
trainer = SFTTrainer(
    model=new_model,
    args=args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['validation'],
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    formatting_func=formatting_prompts_func
)

trainer.train()


output = pipe(messages, **generation_args)
print(output[0]['generated_text'])



