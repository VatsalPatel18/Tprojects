{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2be40b-03f3-4759-a8c2-65e85d7b1ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-bq5t_0lt/unsloth_f42365ebf4194148853abe7311eeefa2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-bq5t_0lt/unsloth_f42365ebf4194148853abe7311eeefa2\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8d9bd0ea8bf662618ba96fe7fe3478c5b81d0dff\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Collecting sentencepiece (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: filelock in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\n",
      "Requirement already satisfied: packaging in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.3.5)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m821.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=109654 sha256=9e25961c81c93f19260129db582d8d8817f8b440d73a2bdac89d2aed291e9bc2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bfnbq3g4/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: sentencepiece, wheel, unsloth, shtab, docstring-parser, tyro\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "Successfully installed docstring-parser-0.16 sentencepiece-0.2.0 shtab-1.7.1 tyro-0.8.4 unsloth-2024.5 wheel-0.43.0\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting trl<0.9.0\n",
      "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: accelerate in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.30.1)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Downloading xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl (222.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0mm\n",
      "\u001b[?25hUsing cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Using cached peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Installing collected packages: bitsandbytes, xformers, trl, peft\n",
      "Successfully installed bitsandbytes-0.43.1 peft-0.11.1 trl-0.8.6 xformers-0.0.26.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ad0c1b-bb92-436f-8f11-8cf73ad22556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "import requests\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cc05fc-3ac9-49d0-886d-93648f8aac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b0f5b9-8a38-46bf-9e32-b45016273e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.811 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e20f1f9-ad59-4e36-a7f6-98304136227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176ba79c-34a3-42ca-9ff4-d72b3421658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 376/376 [00:00<00:00, 13739.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",  # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(batch):\n",
    "    def format_conversation(example):\n",
    "        return [\n",
    "            {\"from\": \"human\", \"value\": example['user_input']},\n",
    "            {\"from\": \"gpt\", \"value\": example['assistant_response']}\n",
    "        ]\n",
    "\n",
    "    texts = [tokenizer.apply_chat_template(format_conversation(example), tokenize=False, add_generation_prompt=False) for example in batch[\"conversations\"]]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load your dataset from JSON file\n",
    "def load_dataset_from_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_dict({\"conversations\": data})\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = 'weather_chatbot_dataset.json'\n",
    "dataset = load_dataset_from_json(dataset_path)\n",
    "\n",
    "# Format the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aadcf94-98ed-48ab-a4f2-ab5399c986ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': {'api_response': {'date': None,\n",
       "   'description': 'clear sky',\n",
       "   'humidity': 65,\n",
       "   'location': 'Warsaw, PL',\n",
       "   'temperature': 18.25,\n",
       "   'wind_speed': 3.6},\n",
       "  'assistant_response': 'The weather in Warsaw is currently clear sky with a temperature of 18.25°C, wind speed of 3.6 meters per second, and humidity of 65%.',\n",
       "  'intent_extraction': {'entities': {'city': 'Warsaw', 'date': 'today'},\n",
       "   'intent': 'current_weather'},\n",
       "  'user_input': 'What is the current weather in Warsaw today?'},\n",
       " 'text': '<s><|user|>\\nWhat is the current weather in Warsaw today?<|end|>\\n<|assistant|>\\nThe weather in Warsaw is currently clear sky with a temperature of 18.25°C, wind speed of 3.6 meters per second, and humidity of 65%.<|end|>\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf6184a-703f-4313-bd6d-4c7ac9b9bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f87b7caf-68d8-4f6f-b3c8-3efa2364e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████████| 376/376 [00:00<00:00, 1676.71 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c9df8-b178-40eb-8dc9-c0d960f588bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1219413-6f6e-45e2-8ad4-1bec269cbb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory = 3.811 GB.\n",
      "2.283 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c460ab63-07ab-4f3d-b196-57344a87c30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 376 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 29,884,416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:32, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f5f39-a47c-452e-a16d-98d00a45f0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32efa079-8e4e-47e3-8a2d-e34a4a2a0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.4404 seconds used for training.\n",
      "2.62 minutes used for training.\n",
      "Peak reserved memory = 2.598 GB.\n",
      "Peak reserved memory for training = 0.315 GB.\n",
      "Peak reserved memory % of max memory = 68.171 %.\n",
      "Peak reserved memory for training % of max memory = 8.266 %.\n"
     ]
    }
   ],
   "source": [
    "# Final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b41a137-bb3f-4e18-94d5-e5dbe280e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inference\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"from\": \"human\", \"value\": \"What is the weather in New York tomorrow?\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "# print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7172bf07-0d62-4470-b7ce-1f9e4490261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Define your weather API key\n",
    "api_key = '337586e7326dcb828d7a386379093040'\n",
    "\n",
    "# Function to get current weather\n",
    "def get_weather(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "# Function to get weather forecast\n",
    "def get_forecast(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={api_key}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "# User input\n",
    "user_input = \"What is the weather in New York tomorrow?\"\n",
    "\n",
    "# Step 1: Intent Identification\n",
    "messages = [{\"from\": \"human\", \"value\": user_input}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "156c804b-7039-455e-866d-10bc83d77686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Intent and Entities: Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8, The next number in the Fibonacci sequence is 13.\n"
     ]
    }
   ],
   "source": [
    "# Generate intent and entity extraction\n",
    "intent_output = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True)\n",
    "intent_response = tokenizer.batch_decode(intent_output, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Identified Intent and Entities:\", intent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "186ead43-0198-4370-852f-0726a26f234e",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m intent_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintent_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m intent \u001b[38;5;241m=\u001b[39m intent_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m city \u001b[38;5;241m=\u001b[39m intent_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "intent_data = json.loads(intent_response)\n",
    "\n",
    "intent = intent_data.get(\"intent\")\n",
    "city = intent_data.get(\"entities\", {}).get(\"city\")\n",
    "date = intent_data.get(\"entities\", {}).get(\"date\")\n",
    "\n",
    "# Step 2: Fetch weather data from API\n",
    "if intent == \"current_weather\":\n",
    "    weather_data = get_weather(city)\n",
    "elif intent == \"forecast_weather\":\n",
    "    weather_data = get_forecast(city)\n",
    "\n",
    "# Generate the response using the weather data\n",
    "if weather_data:\n",
    "    weather_desc = weather_data['weather'][0]['description']\n",
    "    temp = weather_data['main']['temp']\n",
    "    wind_speed = weather_data['wind']['speed']\n",
    "    humidity = weather_data['main']['humidity']\n",
    "\n",
    "    final_response = (\n",
    "        f\"The weather in {city} is currently {weather_desc} with a temperature of {temp}°C, \"\n",
    "        f\"wind speed of {wind_speed} meters per second, and humidity of {humidity}%.\"\n",
    "    )\n",
    "else:\n",
    "    final_response = \"Sorry, I couldn't fetch the weather information.\"\n",
    "\n",
    "print(\"Final Response:\", final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7259af91-8f4d-41c9-ba7d-e30e31e1db8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Generate the final assistant response\u001b[39;00m\n\u001b[1;32m      2\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input},\n\u001b[0;32m----> 4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfinal_response\u001b[49m}\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate the final assistant response\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": user_input},\n",
    "    {\"from\": \"gpt\", \"value\": final_response}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e8b6137-8c9d-4996-b79b-44be85af1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "836857a5-a58c-4338-a438-953e24377d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8, The next number in the Fibonacci sequence is 13.\n",
      "<s><|user|> Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,<|end|><|assistant|> The next number in the Fibonacci sequence is 13.<|end|>\n"
     ]
    }
   ],
   "source": [
    "# Generate the assistant's response\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True)\n",
    "response_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Assistant Response:\", response_text)\n",
    "\n",
    "# Continuous inference example\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f13891d-4e20-4a63-9f56-0a5c37067e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ded594d2-d560-4342-a286-19f07b3ff054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Continuous inference\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"from\": \"human\", \"value\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)\n",
    "\n",
    "# # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfeca4-6bb6-46ba-b661-6422e76dbe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b7f0b-ed99-4910-8aac-0f9d8c69a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0bd57b2-804b-4fe8-a8f6-859fca66841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16112fc9-76f9-4da1-aa23-a0a4775d0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/vatsal-patel/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc8bdd04-bae4-4e6b-b800-fba296156a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a88c134-1f96-4fac-8998-0caf328b263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00002-of-00002.safetensors:   1%|    | 36.2M/3.64G [00:08<09:02, 6.65MB/s]Exception ignored in: <function Dataset.__del__ at 0x70e655b3a320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1421, in __del__\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your repository details\n",
    "repo_name = \"phi3-mini-WeatherBot\"\n",
    "repo_id = f\"VatsalPatel18/{repo_name}\"\n",
    "local_dir = \"./unsloth/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Authenticate using the token from the environment variable\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    raise ValueError(\"The Hugging Face token is not set in the environment variable.\")\n",
    "\n",
    "# # Initialize the HfApi object\n",
    "# api = HfApi()\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "create_repo(repo_id, token=token, exist_ok=True)\n",
    "\n",
    "# Upload each file in the local directory\n",
    "for root, dirs, files in os.walk(local_dir):\n",
    "    for file in files:\n",
    "        if file.startswith(\".\"):\n",
    "            # Skip hidden files and folders\n",
    "            continue\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        repo_file_path = os.path.relpath(local_file_path, local_dir)\n",
    "        upload_file(\n",
    "            path_or_fileobj=local_file_path,\n",
    "            path_in_repo=repo_file_path,\n",
    "            repo_id=repo_id,\n",
    "            token=token\n",
    "        )\n",
    "\n",
    "print(\"Files uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa135b-a451-4f6f-a583-1d5192afd5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15792c6-bd43-4db3-a0bc-5ad32f38dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# Saving to GGUF format\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f88feb-8ef8-44e3-b5d3-2bdddb290334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef00f4-c7f9-4383-96b7-9985867dbc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf9205-bff4-42c3-ab79-ad5520dec6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
