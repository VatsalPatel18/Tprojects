{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's craft an effective dataset format for fine-tuning your LLM to excel at intent identification and location/information extraction for your weather chatbot.\n",
    "\n",
    "Recommended Dataset Format\n",
    "\n",
    "I recommend a structured format using either CSV (Comma-Separated Values) or JSON (JavaScript Object Notation) to store your training examples. Each example in the dataset should consist of the following fields:\n",
    "\n",
    "text: The raw user query (string)\n",
    "intent: The identified intent of the query (string). Examples of intents for your weather chatbot could be:\n",
    "get_current_weather\n",
    "get_forecast\n",
    "get_temperature\n",
    "get_wind_speed\n",
    "get_humidity\n",
    "location: The city or location mentioned in the query (string)\n",
    "weather_info: (Optional) The specific weather information requested (string). This is only needed for some intents, such as get_temperature. Examples could be:\n",
    "temperature\n",
    "wind_speed\n",
    "humidity\n",
    "forecast\n",
    "all (for getting all available information)\n",
    "CSV Example\n",
    "text,intent,location,weather_info\n",
    "What's the weather like in London today?,get_current_weather,London,\n",
    "Tell me the forecast for New York for the next week.,get_forecast,New York,\n",
    "How hot is it in Tokyo right now?,get_temperature,Tokyo,temperature\n",
    "Is it windy in Chicago?,get_wind_speed,Chicago,wind_speed\n",
    "What's the humidity level in Mumbai?,get_humidity,Mumbai,humidity\n",
    "Give me all the weather details for Paris.,get_current_weather,Paris,all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments,AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ab332bd85d42bcab0b6b272442fe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load Your Dataset (Assuming CSV or JSON format)\n",
    "dataset = load_dataset(\"csv\", data_files=\"gemini_tempdata.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'intent', 'location', 'weather_info'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f63bd6c4e8e4beea26cd1230cc88198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17f7c3592a440418b04a98836e19701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Load and Adapt the Phi-3 Mini Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,  # Adjust the rank for efficiency vs. performance tradeoff\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # You can experiment with these\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 4. Set up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU/CPU resources\n",
    "    gradient_accumulation_steps=4,  # Increase if you face OOM issues\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,  # You can experiment with different values\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# 5. Train the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],  # Assuming you have a 'train' split in your dataset\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 6. Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./fine_tuned_phi3\")  # Save in a suitable directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
